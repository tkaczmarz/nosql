## Moje Dane

Wykonać pull request z wpisanym swoim dużym zbiorem danych
(co najmniej 1 mln dokumentów). Link do swoich danych należy wpisać w pliku
link:my_data.md[my_data.md].

Następnie:

. Wybrane dane należy zapisać w bazie danych MongoDB:
.. standalone
.. replica set na localhost
.. replica set na trzech komputerach w laboratorium; można też
  uruchomić replica set trzech innych komputerów, np. na laptopach innych
  studentów. Oczywiście tylko jeśli wyrażą na to zgodę. Wtedy wystarczy
  jeden opis.
. Do zapisu należy użyć programu _mongoimport_.
. Dane importujemy do replica set przy pięciu różnych ustawieniach
  https://docs.mongodb.com/manual/reference/write-concern/[write concern]:
.. domyślnych (należy sprawdzić jakie to są ustawienia)
.. w: 1, j: false
.. w: 1, j: true
.. w: 2, j: false
.. w: 2, j: true
. Import do bazy standalone wykonujemy tylko przy domyslnych ustawieniach.
  Oczywiście należy też sprawdzić jakie one są.

Każdy import powtarzamy pięć razy, liczymy średnie czasy, wielkości kolekcji.
Wyniki przedstawiamy w tabelce.

Dla porównania powtarzamy cały proces dla wzorcowych punktów adresowych
dla **województwa mazowieckiego**.

Czy jest sens porównywania czasów importów / wielkości kolekcji
dla obu zbiorów danych? Jeśli tak, to jak?


## Uwaga

Cały proces opisany poniżej należy zautomatyzować, łącznie z wygenerowaniem
tabelki. Wyniki – czyli czasy, wielkości kolekcji i wnioski – należy opisać
w notacji [Asciidoctor](https://asciidoctor.org/docs/) w pliku _README.adoc_
w repozytorium na GitHub Classroom.
